{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de Ejercicios y Problemas tipo examen 2\n",
    "Ismael Sagredo Olivenza\n",
    "Colaboración en este material de Belén Díaz Agudo y otros profesores de ISIA\n",
    "## Problema 01\n",
    "Tenemos un dataset con datos almacenados sobre las precipitaciones desde hace 10\n",
    "años y queremos predecir el nivel de precipitación en el futuro en función del dia del\n",
    "año (entero), la humedad (real), la cantidad de nubosidad (baja, media, alta), la\n",
    "temperatura (real) y la presión atmosférica (real).\n",
    "Hemos aplicado diferentes técnicas de Machine Learning y nos ha generado los\n",
    "siguientes resultados de accuracy:\n",
    "\n",
    "- KNN: K=5, 88% distancia euclídea.\n",
    "- Decision Tree: 90%, profundidad del árbol 5.\n",
    "- Random Forest: 98%\n",
    "- Perceptrón Multicapa: 10 neuronas ocultas en la primera capa y 5 en la segunda,\n",
    "factor de aprendiszaje de 0,2, valores normalizados 0-1: 90%\n",
    "\n",
    "A los investigadores les ha sorprendido que el perceptrón multicapa de resultados tan\n",
    "bajos.\n",
    "\n",
    "1. ¿Qué podriamos hacer para mejorar los resultados del perceptrón?\n",
    "2. Con los datos del ejercicio, razona cual sería el modelo que elegirías.\n",
    "3. En KNN el valor categórico ha sido codificado como 0, 1, 2 y el resto de atributos\n",
    "para calcular la diferencia se realizan restando sus valores sin ninguna\n",
    "transformación. ¿Cómo podríamos mejorar KNN para conseguir mejores\n",
    "resultados?\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "1. Para mejorar los resultados del perceptrón multicapa, podríamos considerar las siguientes estrategias:\n",
    "   - Ajustar la arquitectura de la red: Experimentar con diferentes configuraciones de capas y unidades ocultas puede influir significativamente en el rendimiento del perceptrón.\n",
    "   - Tuning de hiperparámetros: Ajustar el factor de aprendizaje, el número de épocas y otros hiperparámetros del modelo puede mejorar la convergencia y el rendimiento general.\n",
    "   - Normalización de datos: Asegurarse de que los datos estén correctamente normalizados puede ser crucial para el rendimiento del perceptrón multicapa. Asegúrate de que todas las características estén en una escala similar (por ejemplo, entre 0 y 1).\n",
    "   - Regularización: Introducir técnicas de regularización como la penalización L1 o L2 puede ayudar a prevenir el sobreajuste y mejorar la generalización del modelo.\n",
    "\n",
    "2. Basándonos en los resultados proporcionados:\n",
    "   - El modelo Random Forest muestra el mayor rendimiento con un 98% de precisión, lo que sugiere una buena capacidad de generalización y robustez.\n",
    "   - El Decision Tree también tiene un rendimiento sólido con un 90% de precisión.\n",
    "   - KNN tiene un rendimiento aceptable, pero puede beneficiarse de ajustes en la elección de K y la métrica de distancia.\n",
    "   - El Perceptrón Multicapa, a pesar de su rendimiento más bajo del 90%, podría mejorarse mediante la exploración de las estrategias mencionadas anteriormente.\n",
    "\n",
    "3. Para mejorar KNN y obtener mejores resultados, podríamos considerar:\n",
    "   - Optimización de K: Experimentar con diferentes valores de K para encontrar el óptimo para el conjunto de datos particular.\n",
    "   - Selección de métrica de distancia: Probar otras métricas de distancia además de la distancia euclidiana, como la distancia de Mahalanobis o la distancia de Minkowski.\n",
    "   - Normalización de atributos: Normalizar los atributos puede ser crucial en KNN, ya que las diferencias en la escala de los atributos pueden afectar significativamente a las distancias calculadas.\n",
    "   - Evaluación de ponderación: Algunas implementaciones de KNN permiten ponderar las contribuciones de los vecinos en función de su distancia; experimentar con esto puede mejorar el rendimiento.\n",
    "\n",
    "### Respuesta Isma\n",
    "1. ¿Qué podriamos hacer para mejorar los resultados del perceptrón?\n",
    "\n",
    "El factor de aprendizaje quizás esté muy alto, podríamos reentrenar la red con otro\n",
    "factor de aprendizaje. Tambien podríamos modificar la arquitectura de la red o\n",
    "seleccionar los atributos\n",
    "\n",
    "2. Con los datos del ejercicio, razona cual sería el modelo que elegirías.\n",
    "\n",
    "La respuesta peude ser variada y si está argumentada sería correcta. Por un lado\n",
    "podemos pensar que un 90% es suficiente y elegir arboles de decisión por ser un\n",
    "algoritmo de caja blanca. Tenemos que tener en cuenta que saber si va a llover o no es\n",
    "algo que tiene importancia para hoteles o para saber si te vas de vacaciones o no, pero\n",
    "si 9 de cada 10 veces se va a acertar, no es un mal rendimiento. Si lo que queremos es\n",
    "maximizar la precisión usaríamos random forest, pero tenemos que ser conscientes\n",
    "que perdemos explicabilidad.\n",
    "\n",
    "3. En KNN el valor categórico ha sido codificado como 0, 1, 2 y el resto de atributos\n",
    "para calcular la diferencia se realizan restando sus valores sin ninguna\n",
    "transformación. ¿Cómo podríamos mejorar KNN para conseguir mejores\n",
    "resultados?\n",
    "\n",
    "Podriamos aplicar one-hot-enconding para codificar los valores categóricos. Tambien\n",
    "estaria bien normalizar el resto de valores. Bien aplicando una resta de la normal y\n",
    "dividiendo por la desviación típica o bien haciendo que todos los datos esten entre 0 y 1.\n",
    "Esto haría que la distancia euclídea represente mejor al individuo.\n",
    "\n",
    "### Claves de la respuesta\n",
    "1. Jugar con la arquitectura del modelo, el factor de aprendizaje, asegurar normalización correcta, y usar regularización L2 para evitar sobreajustes.\n",
    "2. Si nos interesa sólo la precisión, usar Random Forest. Si queremos ver el proceso de decisión, usar Decision Tree.\n",
    "3. Usar one-hot-encoding y normalización.\n",
    "\n",
    "## Problema 2\n",
    "En un problema de Machine Learning, hemos utilizado la librería train_test_split para\n",
    "dividir el conjunto de entrenamiento y de test. En concreto se ha utilizado con los\n",
    "siguientes valores train_test_split(data, y, train_size = 0.8, random_state = 1)\n",
    "Sin embargo, haciendo pruebas vemos que en función del random_state o del tamaño\n",
    "de los datos de entrenamiento el resultado varia hasta un 5% de precisión.\n",
    "1. ¿Cómo podríamos abordar este problema?\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "La variabilidad en los resultados del modelo de Machine Learning debido a la aleatoriedad en la división de los conjuntos de entrenamiento y prueba puede abordarse de varias maneras:\n",
    "\n",
    "1. **Fijar la Semilla (Random State):** Si deseas obtener resultados reproducibles, puedes fijar la semilla (random_state) a un valor específico. Esto garantiza que, cada vez que ejecutas el código, obtendrás la misma división de conjuntos de entrenamiento y prueba. Por ejemplo, `random_state = 1`.\n",
    "\n",
    "   ```python\n",
    "   train_test_split(data, y, train_size=0.8, random_state=1)\n",
    "   ```\n",
    "\n",
    "   Asegúrate de documentar y compartir la semilla utilizada para que otros puedan reproducir los resultados.\n",
    "\n",
    "2. **Validación Cruzada (Cross-Validation):** En lugar de depender de una sola división de entrenamiento/prueba, puedes utilizar técnicas de validación cruzada. K-Fold Cross-Validation implica dividir los datos en k particiones, entrenar y evaluar el modelo k veces, utilizando diferentes particiones como conjunto de prueba en cada iteración. Esto proporciona una evaluación más robusta del rendimiento del modelo.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.model_selection import cross_val_score\n",
    "\n",
    "   scores = cross_val_score(model, data, y, cv=5)  # Ejemplo con k=5\n",
    "   ```\n",
    "\n",
    "3. **Ajuste del Tamaño del Conjunto de Entrenamiento:** El tamaño del conjunto de entrenamiento también puede influir en los resultados. Puedes experimentar con diferentes tamaños de conjunto de entrenamiento y evaluar cómo afecta al rendimiento. Asegúrate de tener suficientes datos en el conjunto de prueba para obtener una evaluación significativa del rendimiento del modelo.\n",
    "\n",
    "   ```python\n",
    "   train_test_split(data, y, train_size=0.7, random_state=1)  # Ejemplo con un 70% de entrenamiento\n",
    "   ```\n",
    "\n",
    "4. **Análisis Detallado:** Realizar un análisis detallado de por qué los resultados varían. Puede haber patrones particulares en los datos que se manifiesten solo en ciertas divisiones. Asegúrate de explorar las características de los datos y entender si hay algún sesgo o patrón que pueda explicar las diferencias en la precisión.\n",
    "\n",
    "Al implementar estas estrategias, puedes reducir la variabilidad en los resultados y obtener una evaluación más robusta del rendimiento del modelo.\n",
    "\n",
    "### Respuesta Isma\n",
    "La mejor forma de abordar este problema es usando cross validation. Con esta técnica\n",
    "calculamos el error medio entre diferentes particiones del dataset entre datos de\n",
    "entrenamiento y de validación, lo que minimiza la desviación que puede producir una\n",
    "mala partición del dataset entre entrenamiento y validación.\n",
    "\n",
    "### Claves de la respuesta\n",
    "Quiere que le expliques Cross-validation, ya que sirve precisamente para estos casos.\n",
    "\n",
    "## Pregunta 3\n",
    "Queremos detectar a partir de imágenes de rayos X si existe o no un tumor cerebral.\n",
    "Para ello disponemos de un dataset con 100.000 imágenes de 1024x1024 en escala de\n",
    "grises.\n",
    "\n",
    "1. Explica los pasos que harías para preparar los datos\n",
    "2. A priori, cual crees que sería el mejor modelo y porqué.\n",
    "3. Suponiendo que se han creado dos modelos y con 2000 datos de validación, el\n",
    "modelo A y el modelo B, ¿qué modelo elegirías?\n",
    "\n",
    "La matriz de confusión de A es:\n",
    "\n",
    "|          | Tumor | No-tumor |\n",
    "|----------|-------|----------|\n",
    "| **Tumor**   | 800   | 200      |\n",
    "| **No-tumor**| 100   | 900      |\n",
    "\n",
    "La matriz de confusión de B es:\n",
    "\n",
    "|          | Tumor | No-tumor |\n",
    "|----------|-------|----------|\n",
    "| **Tumor**   | 900   | 100      |\n",
    "| **No-tumor**| 200   | 800      |\n",
    "\n",
    "### Respuesta ChatGpt\n",
    "1. **Pasos para preparar los datos:**\n",
    "   - **Exploración de datos:** Analizar algunas imágenes para entender su formato y características. Verificar si hay desequilibrios en las clases.\n",
    "   - **Preprocesamiento:** Redimensionar las imágenes a un tamaño manejable, normalizar los valores de píxeles (escala de 0 a 1), y realizar cualquier ajuste necesario para mejorar la calidad de las imágenes.\n",
    "   - **Etiquetado:** Asignar etiquetas a las imágenes según la presencia o ausencia de un tumor cerebral.\n",
    "   - **División del conjunto de datos:** Separar los datos en conjuntos de entrenamiento, validación y prueba.\n",
    "   - **Data Augmentation (si es necesario):** Generar versiones adicionales de las imágenes mediante rotación, inversión, zoom, etc., para aumentar la variabilidad del conjunto de entrenamiento.\n",
    "   \n",
    "2. **Elección del mejor modelo:**\n",
    "   - Dada la naturaleza de las imágenes médicas, las redes neuronales convolucionales (CNN) son comúnmente utilizadas para tareas de clasificación de imágenes. Por lo tanto, un modelo de CNN sería una elección natural para este problema. Además, las CNN son eficientes para aprender patrones espaciales en imágenes.\n",
    "\n",
    "3. **Comparación de modelos A y B:**\n",
    "   - **Modelo A:**\n",
    "     - Verdaderos positivos (TP): 800\n",
    "     - Verdaderos negativos (TN): 900\n",
    "     - Falsos positivos (FP): 200\n",
    "     - Falsos negativos (FN): 100\n",
    "     - Precisión = (TP + TN) / (TP + TN + FP + FN) = (800 + 900) / (800 + 900 + 200 + 100) ≈ 0.86\n",
    "   - **Modelo B:**\n",
    "     - Verdaderos positivos (TP): 900\n",
    "     - Verdaderos negativos (TN): 800\n",
    "     - Falsos positivos (FP): 100\n",
    "     - Falsos negativos (FN): 200\n",
    "     - Precisión = (TP + TN) / (TP + TN + FP + FN) = (900 + 800) / (900 + 800 + 100 + 200) ≈ 0.85\n",
    "\n",
    "   Ambos modelos tienen una precisión similar, pero la elección entre ellos podría depender de otros factores como la sensibilidad (recall), la especificidad, el F1-score, etc. También, podrías considerar la interpretación clínica y los requisitos específicos del problema.\n",
    "\n",
    "### Respuesta Isma\n",
    "1. Explica los pasos que harías para preparar los datos\n",
    "\n",
    "Las imágenes son demasiado grandes habría que escalarlas a un tamaño más\n",
    "manejable, como por ejemplo 64x64. Podemos escalarlas a mano usando alguna\n",
    "algoritmos de escaldo, o podriamos intantar extraer las caracteristicas de una imagen\n",
    "como etapa de preprocesado inicial. Para ello podemos utilizar un Autoencoder o un\n",
    "algortimo de PCA. Por otro lado usando PCA dibujaría las diferentes clases para ver\n",
    "que pinta tiene. Tambien se podría hacer un análisis de las imágenes y detectar que\n",
    "pixeles aportan más información y hasta que punto podemos recortar las imágenes.\n",
    "\n",
    "2. A priori, cual crees que sería el mejor modelo y porqué.\n",
    "\n",
    "A priori el mejor modelo para procesar imágenes de los que hemos estudiado son las\n",
    "redes de neuronas convolucionales y los autoencoders. Tambien a priori el perceptrón\n",
    "multicapa se comporta mejor en este tipo de problemas que otros algoritmos. Pero no\n",
    "podemos asegurarlo a ciencia cierta hasta hacer pruebas.\n",
    "\n",
    "3. Suponiendo que se han creado dos modelos y con 2000 datos de validación, el\n",
    "modelo A y el modelo B, ¿qué modelo elegirías?\n",
    "\n",
    "El modelo B ya que es más importante que no haya falsos positivos, es decir que\n",
    "habiendo un tumor se clasifique como no tumor. Es peor este resultado que clasificar\n",
    "como tumor algo que no lo tiene, ya que normalmente se le realizaran otras pruebas\n",
    "antes de darle un diagnostico final.\n",
    "\n",
    "### Claves de la respuesta\n",
    "1. Las imagenes deben reducirse en tamaño y complejidad, ya sea reescalándolas de\n",
    "forma manual o usando convolución, autoencoders o PCA.\n",
    "2. Las redes neuronales convolucionales se especializan en imágenes.\n",
    "3. Este problema es serio y es muy importante detectar los positivos. \n",
    "Fallar en los negativos nos importa menos.\n",
    "\n",
    "## Problema 4\n",
    "Hemos conectado al videojuego Tetris un módulo que captura los eventos del mando,\n",
    "que se van registrando a la vez que se va haciendo una foto del estado del jugador. La\n",
    "idea es construir un sistema que permite jugar de forma autónoma a un agente que\n",
    "debe jugar igual que el jugador para el modo versus, donde un jugador juega contra\n",
    "una IA. Para ellos almacenamos en un histórico el estado del juego y la acción que ha\n",
    "realizado el jugador que puede ser una de las siguientes:\n",
    "\n",
    "- Mover Izquierda (MI)\n",
    "- Mover derecha (MD)\n",
    "- Rotar pieza (R)\n",
    "- Bajar pieza (B)\n",
    "- Guardar pieza (G)\n",
    "\n",
    "El estado está compuesto por una matriz con las celdas ocupadas por piezas, el nivel\n",
    "actual, la posición de la pieza que cae actualmente, la siguiente pieza y la pieza\n",
    "almacenada de repuesto.\n",
    "\n",
    "Si queremos utilizar KNN para seleccionar la acción más similar a la realizada por el\n",
    "usuario….\n",
    "\n",
    "1. ¿Como codificarías los datos para que KNN pueda funcionar correctamente y que\n",
    "medida de distancia usarías?\n",
    "2. Para saber si el agente imita correctamente al jugador, comprobamos que la\n",
    "secuencia de acciones realizadas por el agente sea similar a la del jugador para un\n",
    "estado de juego dado (El Tetris es determinista asumiendo la misma semilla\n",
    "aleatoria) De esta forma tendremos la secuencia de acciones MI|MD|R|B|MI|MI|B|G|MD|MI|MD|R|R|B.\n",
    "¿Qué medida de distancia utilizarías para calcular el error?\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "1. **Codificación de Datos para KNN:**\n",
    "   - Cada estado del juego y acción realizada por el jugador debe ser codificado en un formato numérico para que KNN pueda operar. Puedes utilizar un enfoque de \"one-hot encoding\" para las acciones y convertir el estado del juego en un vector unidimensional, incluyendo todos los elementos relevantes (matriz de celdas ocupadas, nivel, posición de la pieza actual, siguiente pieza, pieza almacenada).\n",
    "\n",
    "   - Por ejemplo, si tienes cinco acciones posibles (MI, MD, R, B, G), podrías codificarlas como [1, 0, 0, 0, 0] para MI, [0, 1, 0, 0, 0] para MD, y así sucesivamente. Concatena esta representación con el vector que describe el estado del juego.\n",
    "\n",
    "   - Respecto a la medida de distancia, la elección de la distancia dependerá de la naturaleza de tus datos y la importancia relativa de cada característica. La distancia euclidiana es común, pero también podrías considerar otras métricas como la distancia de Manhattan o la distancia de Minkowski.\n",
    "\n",
    "2. **Medida de Distancia para Calcular el Error:**\n",
    "   - Dado que estás tratando con una secuencia de acciones, la distancia entre dos secuencias puede evaluarse mediante la distancia de edición (también conocida como distancia de Levenshtein). Esta medida cuenta el número mínimo de operaciones necesarias para transformar una secuencia en la otra, donde las operaciones son inserciones, eliminaciones o sustituciones de elementos.\n",
    "\n",
    "   - En tu caso, podrías usar la distancia de edición para comparar la secuencia de acciones realizadas por el jugador con la secuencia de acciones generadas por el agente. Cuanto menor sea la distancia de edición, más similares son las secuencias.\n",
    "\n",
    "   - La distancia de edición se adapta bien a comparaciones de secuencias y puede ser adecuada para evaluar el rendimiento del agente en términos de imitar la secuencia de acciones del jugador en un estado de juego dado.\n",
    "\n",
    "### Respuesta Isma\n",
    "1. ¿Como codificarías los datos para que KNN pueda funcionar correctamente y que\n",
    "medida de distancia usarías?\n",
    "\n",
    "- La cuadrícula con piezas seria una matriz de entrada donde valdría 1 la casilla X,Y\n",
    "esta ocupada por una pieza y 0 en caso contrario\n",
    "- La posición de la pieza actual se guardará como un valor real con el origen de\n",
    "coordinadas de (0,0) en la esquina inferior izquierda de la parte jugable y que ira\n",
    "de 0 a 1.\n",
    "- Usamos One-hot-encoding para guardar el identificador de la pieza guardada, la\n",
    "actual y la próxima ya que la categoria no es ordinal\n",
    "- Para saber el nivel tambien codificaremos este número como número real entre\n",
    "0-1 seindo 0 el nivel 0 y 1 el nivel más alto posible.\n",
    "- como medida de similitud usaría la distancia de Manhattan ya que en este tipo de\n",
    "problemas las diagonales no son importantes a priori (en el tetris no te puedes\n",
    "mover en diagonal)\n",
    "- Tendría una ventana temporal de acciones realizadas previamente ya que hay\n",
    "una dependencia entre las acciones realizadas anteriormente y las nuevas.\n",
    "\n",
    "2. Para saber si el agente imita correctamente al jugador, comprobamos que la\n",
    "secuencia de acciones realizadas por el agente sea similar a la del jugador para un\n",
    "estado de juego dado (El Tetris es determinista asumiendo la misma semilla\n",
    "aleatoria) De esta forma tendremos la secuencia de acciones MI|MD|R|B|MI|MI|B|G|MD|MI|MD|R|R|B.\n",
    "¿Qué medida de distancia utilizarías para calcular el error?\n",
    "\n",
    "Usaría distancia de edición. La similitud sería el número de cambios que hacen que\n",
    "una cadena sea igual que la otra. Esta medida es más representativa que una distancia\n",
    "euclídea, primero porque el valor de la clase no es ordinal y por otro lado, porque hay\n",
    "una dependencia temporal de las acciones realizadas. Si queremos imitar a la\n",
    "percepción el comportamiento del humano, debemos realizar las mismas acciones en\n",
    "le mismo orden más allá de que estas sean buenas o malas.\n",
    "\n",
    "### Claves de la respuesta\n",
    "1. Representar el estado con unos datos con sentido.\n",
    "2. Para cadenas y secuencias se usa la distancia de edición, también llamada distancia de Levenshtein.\n",
    "Mide el número de cambios que hay que hacer para convertir una instancia en otra (casa → cala → calla → calle)\n",
    "\n",
    "## Problema 5\n",
    "Dada la siguiente tabla con los datos de entrenamiento de un algoritmo ID3 y\n",
    "utilizando la Ganancia y la Entropía\n",
    "Claro, aquí tienes el texto formateado en Markdown:\n",
    "\n",
    "Dada la siguiente tabla con los datos de entrenamiento de un algoritmo ID3 y utilizando la Ganancia y la Entropía:\n",
    "\n",
    "\\[ \\text{Entropía}(s) = \\sum_{i}^{-p_i \\cdot \\log_2{p_i}} \\]\n",
    "\n",
    "\\[ \\text{Ganancia}(S, A) = \\text{Entropía}(S) - \\sum_{v \\in V(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropía}(S_v) \\]\n",
    "\n",
    "Donde 𝑝𝑖 es la probabilidad del atributo i-ésimo, S es el conjunto de todos los ejemplos\n",
    "y A el conjunto de todos los atributos y V(A) los valores de todos los atributos.\n",
    "\n",
    "| A1 | A2 | A3 | A4 | Clase |\n",
    "|----|----|----|----|-------|\n",
    "| 2  | 0  | 3  | A  | Bueno |\n",
    "| 1  | 1  | 1  | B  | Malo  |\n",
    "| 1  | 2  | 0  | B  | Bueno |\n",
    "| 0  | 2  | 1  | B  | Bueno |\n",
    "| 2  | 1  | 0  | A  | Malo  |\n",
    "| 3  | 0  | 1  | B  | Malo  |\n",
    "\n",
    "1. Calcular cuál será el atributo que primero elegirá ID3\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "Para calcular la Ganancia de Información y la Entropía para decidir cuál atributo elegirá primero el algoritmo ID3, sigamos los pasos. Primero, calculemos la Entropía de S (Entropía(S)) y luego la Ganancia de Información para cada atributo (Ganancia(S, A)):\n",
    "\n",
    "1. **Calcular Entropía(S):**\n",
    "\n",
    "\\[ \\text{Entropía}(S) = -p_{\\text{Bueno}} \\cdot \\log_2(p_{\\text{Bueno}}) - p_{\\text{Malo}} \\cdot \\log_2(p_{\\text{Malo}}) \\]\n",
    "\n",
    "\\[ p_{\\text{Bueno}} = \\frac{3}{6} \\]\n",
    "\\[ p_{\\text{Malo}} = \\frac{3}{6} \\]\n",
    "\n",
    "\\[ \\text{Entropía}(S) = -\\frac{3}{6} \\cdot \\log_2\\left(\\frac{3}{6}\\right) - \\frac{3}{6} \\cdot \\log_2\\left(\\frac{3}{6}\\right) \\]\n",
    "\n",
    "2. **Calcular Ganancia de Información para cada atributo (A1, A2, A3, A4):**\n",
    "\n",
    "\\[ \\text{Ganancia}(S, A) = \\text{Entropía}(S) - \\sum_{v \\in V(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropía}(S_v) \\]\n",
    "\n",
    "   - Para A1:\n",
    "\n",
    "     \\[ |S_1| = 1, |S_2| = 2, |S_3| = 2, |S_4| = 1 \\]\n",
    "\n",
    "     \\[ \\text{Ganancia}(S, A1) = \\text{Entropía}(S)\n",
    "\n",
    "### Respuesta Isma\n",
    "\n",
    "1. ¿Qué atributo elegiría primero el árbol de decisión?\n",
    "La fórmula para calcular el logaritmo en base 2 se expresa como:\n",
    "\n",
    "\\[ \\log_2{N} = \\frac{\\log_b{N}}{\\log_b{2}} \\quad \\forall b \\]\n",
    "\n",
    "Por ejemplo, \\(\\log_{10}{A1} = \\frac{\\log_{2}{A1}}{\\log_{2}{10}}\\).\n",
    "\n",
    "Para el atributo A1, para la clase (Bueno) tenemos un 1. Para la clase (malo) tenemos\n",
    "0.\n",
    "\n",
    "- Para el valor de 0: \n",
    "  \\[ I_{10} = -\\frac{1}{1} \\log_2{1} - \\frac{0}{1} \\log_2{0} = 0 \\]\n",
    "- Para el valor de 1 tenemos uno de la clase malo y otro de la clase bueno:\n",
    "  \\[ I_{11} = -\\frac{1}{2} \\log_2{\\frac{1}{2}} - \\frac{1}{2} \\log_2{\\frac{1}{2}} = 1 \\]\n",
    "- Para el valor de 2 tenemos uno de la clase malo y otro de la clase bueno:\n",
    "  \\[ I_{12} = -\\frac{1}{2} \\log_2{\\frac{1}{2}} - \\frac{1}{2} \\log_2{\\frac{1}{2}} = 1 \\]\n",
    "- Para el valor de 3:\n",
    "  \\[ I_{13} = -\\frac{1}{1} \\log_2{1} - \\frac{0}{1} \\log_2{0} = 0 \\]\n",
    "\n",
    "\\[ A1 = \\frac{1}{6} \\cdot (0) + \\frac{2}{6} \\cdot 1 + \\frac{2}{6} \\cdot 1 + \\frac{1}{6} \\cdot 0 = \\frac{4}{6} = 0.66 \\]\n",
    "\n",
    "\\[ I_{20} + I_{21} + I_{22} = \\frac{2}{6} \\cdot 1 + \\frac{2}{6} \\cdot 0 + \\frac{2}{6} \\cdot 0 = \\frac{2}{6} \\]\n",
    "\n",
    "\\[ A2 = \\frac{1}{3} = \\frac{2}{6} \\]\n",
    "\n",
    "\\[ I_{30} + I_{31} + I_{33} = \\frac{2}{6} \\cdot 1 + \\frac{3}{6} \\cdot 0.924 + \\frac{1}{6} \\cdot 0 = 0.33 + 0.462 \\]\n",
    "\n",
    "\\[ A3 = \\frac{5}{6} = \\frac{2}{6} \\]\n",
    "\n",
    "\\[ I_{4A} + I_{4B} = \\frac{2}{6} \\cdot 1 + \\frac{4}{6} \\cdot 1 = \\frac{5}{6} \\]\n",
    "\n",
    "\\[ A4 = 1 = \\frac{2}{6} \\]\n",
    "\n",
    "El atributo que el árbol de decisión elegiría primero sería A2, ya que tiene la menor impureza entre los atributos considerados.\n",
    "\n",
    "### Claves de la respuesta\n",
    "Copiar y rezar xd\n",
    "\n",
    "## Problema 6\n",
    "Si quiero extraer las características de una imagen para poder aplicar un algoritmo de\n",
    "ML sobre una versión reducida de dicha imagen ¿Qué técnicas puedo aplicar y que\n",
    "ventajas e inconvenientes tiene cada una?\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "Para extraer características de una imagen antes de aplicar un algoritmo de Machine Learning sobre una versión reducida, puedes considerar varias técnicas. Aquí hay algunas opciones comunes:\n",
    "\n",
    "1. **Histogramas de Color:**\n",
    "   - **Ventajas:**\n",
    "     - Simple y rápido de calcular.\n",
    "     - Puede proporcionar información sobre la distribución de colores en la imagen.\n",
    "   - **Inconvenientes:**\n",
    "     - No captura la estructura espacial de la imagen.\n",
    "\n",
    "2. **Texturas:**\n",
    "   - **Ventajas:**\n",
    "     - Captura la información sobre la variación local de intensidades en la imagen.\n",
    "     - Útil para detectar patrones repetitivos.\n",
    "   - **Inconvenientes:**\n",
    "     - Puede ser computacionalmente costoso.\n",
    "     - Requiere métodos más avanzados para su cálculo, como matrices de co-ocurrencia.\n",
    "\n",
    "3. **Descriptores de Forma:**\n",
    "   - **Ventajas:**\n",
    "     - Describe la forma y la geometría de los objetos en la imagen.\n",
    "   - **Inconvenientes:**\n",
    "     - Sensible a la rotación y escala.\n",
    "\n",
    "4. **Características de Frecuencia (Transformada de Fourier, Transformada Wavelet):**\n",
    "   - **Ventajas:**\n",
    "     - Puede proporcionar información sobre patrones en diferentes escalas.\n",
    "   - **Inconvenientes:**\n",
    "     - Puede ser computacionalmente intensivo.\n",
    "     - Requiere conocimiento sobre la interpretación de los resultados.\n",
    "\n",
    "5. **Descriptores Locales (por ejemplo, SIFT, SURF, ORB):**\n",
    "   - **Ventajas:**\n",
    "     - Capturan características locales y son robustos a las transformaciones.\n",
    "   - **Inconvenientes:**\n",
    "     - Pueden ser computacionalmente costosos.\n",
    "     - Requieren más recursos y tiempo de cómputo.\n",
    "\n",
    "6. **Redes Neuronales Convolucionales (CNN):**\n",
    "   - **Ventajas:**\n",
    "     - Aprenden automáticamente características jerárquicas.\n",
    "     - Eficientes para tareas de visión por computadora.\n",
    "   - **Inconvenientes:**\n",
    "     - Requieren grandes cantidades de datos de entrenamiento.\n",
    "     - Pueden ser más complejas y difíciles de interpretar.\n",
    "\n",
    "La elección de la técnica dependerá del contexto específico de tu aplicación, la cantidad de datos disponibles, la complejidad del problema y los recursos computacionales disponibles. En muchos casos, una combinación de varias técnicas puede proporcionar la mejor representación de las características de la imagen.\n",
    "\n",
    "### Respuesta Isma\n",
    "-Escalar la imagen usando un algoritmo de escalado: método conocido y rápido\n",
    "desventaja, no permite extraer información más allá de concentrar la información\n",
    "en menos pixeles\n",
    "- PCA:\n",
    "- Autoencoder: Permite extraer un resumen de la red más complejo y más rico,\n",
    "pero se necesita un gran volumen de datos para entrenar\n",
    "- Convolutional networks: permite detectar características concretas dentro de\n",
    "una imagen. Solo detecta una característica concreta por cada capa convolucional.\n",
    "no reduce la dimensionalidad tanto como autoenconder y pca\n",
    "\n",
    "### Respuesta Diego\n",
    "- Escalar la imagen usando un algoritmo de escalado:\n",
    "    - Ventajas: método conocido y rápido.\n",
    "    - Desventajas: no permite extraer información más allá de concentrar la\n",
    "información en menos píxeles.\n",
    "- PCA:\n",
    "    - Ventajas: Reduce el número de variables manteniendo la mayor parte de\n",
    "la información. Es útil para la visualización de datos y puede mejorar el\n",
    "rendimiento de los algoritmos de ML al eliminar el ruido y las\n",
    "características irrelevantes.\n",
    "    - Desventajas: La reducción puede resultar en la pérdida de información\n",
    "importante. La interpretación de los componentes principales puede ser\n",
    "difícil.\n",
    "- Redes neuronales Convolucionales (CNN):\n",
    "    - Ventajas: Excelente para extraer características jerárquicas\n",
    "automáticamente. Muy efectivo en tareas de reconocimiento y\n",
    "clasificación de imágenes.\n",
    "    - Desventajas: Requiere una gran cantidad de datos y recursos\n",
    "computacionales. La interpretación de las características extraídas\n",
    "puede ser compleja.\n",
    "- Autoencoders:\n",
    "    - Ventajas: Aprenden a comprimir y descomprimir datos, capturando\n",
    "características esenciales. Útil para reducción de dimensionalidad y\n",
    "aprendizaje no supervisado.\n",
    "    - Desventajas: Puede ser difícil entrenar y ajustar. La calidad de las\n",
    "características extraídas depende en gran medida de la arquitectura y\n",
    "los datos de entrenamiento.\n",
    "\n",
    "### Claves de la respuesta\n",
    "Data Agmentation, escalar la muestra para reducir el tamaño (manualmente o con PCA),\n",
    "las redes convolucionales se especializan en imagenes. El autoencoder extrae\n",
    "las características de una imagen codificadas en muchos menos componentes.\n",
    "\n",
    "## Problema 7\n",
    "A priori (ya que no se está totalmente seguro hasta no implementarlos), ¿Qué\n",
    "algoritmo clasificaría mejor un problema que no es linealmente separable? Explica\n",
    "porqué.\n",
    "- Un árbol de decisión\n",
    "- Una Red de neuronas profunda compuesta únicamente por capas\n",
    "convolucionales.\n",
    "- Un Perceptrón multicapa con función de activación ReLu\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "En un problema que no es linealmente separable, es probable que una Red de Neuronas Profunda (DNN) compuesta únicamente por capas convolucionales tenga un mejor rendimiento que un árbol de decisión o un perceptrón multicapa con función de activación ReLu. Aquí están las razones:\n",
    "\n",
    "1. **Árbol de Decisión:**\n",
    "   - *Ventajas:*\n",
    "     - Buen rendimiento en problemas con separación lineal o estructura jerárquica simple.\n",
    "     - Interpretabilidad, ya que puede proporcionar reglas de decisión fáciles de entender.\n",
    "   - *Inconvenientes:*\n",
    "     - Limitado en la capacidad para aprender patrones complejos y no lineales.\n",
    "     - Sensible a variaciones pequeñas en los datos de entrenamiento.\n",
    "\n",
    "2. **Red de Neuronas Profunda (Solo Capas Convolucionales):**\n",
    "   - *Ventajas:*\n",
    "     - Capaz de aprender representaciones jerárquicas y patrones complejos.\n",
    "     - Robusta para manejar datos no lineales y problemas de visión por computadora.\n",
    "     - Puede aprender características espaciales locales a través de las capas convolucionales.\n",
    "   - *Inconvenientes:*\n",
    "     - Requiere grandes cantidades de datos de entrenamiento.\n",
    "     - Puede ser computacionalmente costoso y requerir recursos de hardware.\n",
    "\n",
    "3. **Perceptrón Multicapa con Función de Activación ReLu:**\n",
    "   - *Ventajas:*\n",
    "     - Puede aprender representaciones no lineales.\n",
    "     - Bien adaptado para problemas complejos.\n",
    "     - Eficiente en términos computacionales.\n",
    "   - *Inconvenientes:*\n",
    "     - No es tan eficaz como las CNN en la captura de características espaciales locales.\n",
    "     - Puede tener dificultades con datos de alta dimensionalidad, como imágenes.\n",
    "\n",
    "En resumen, una red de neuronas profundas, especialmente aquellas diseñadas específicamente para problemas de visión por computadora con capas convolucionales, generalmente tiene un rendimiento superior en problemas no lineales y en la clasificación de datos no linealmente separables, como en el caso de imágenes. Sin embargo, la elección del algoritmo también depende de otros factores, como la disponibilidad de datos de entrenamiento, el tamaño del conjunto de datos y los recursos computacionales disponibles.\n",
    "\n",
    "### Respuesta Isma\n",
    "Un Perceptrón multicapa con función de activación ReLu ya que no es una función de\n",
    "activación lineal\n",
    "\n",
    "### Respuesta Diego\n",
    "Mientras que los árboles de decisión y las CNN tienen sus fortalezas en ciertos tipos de\n",
    "datos y problemas específicos, un Perceptrón Multicapa con función de activación\n",
    "ReLu generalmente ofrece una mayor flexibilidad y capacidad para modelar problemas\n",
    "complejos y no lineales. Esto se debe a su arquitectura de múltiples capas y a la\n",
    "capacidad de la función ReLu para introducir no linealidades, lo que permite que el\n",
    "MLP aprenda patrones complejos y relaciones intrincadas en los datos.\n",
    "\n",
    "### Clave de la respuesta\n",
    "Decir redes convolucionales si es para imagen, pero como te esta especificando relu quieres elegir esa.\n",
    "\n",
    "## Problema 8\n",
    "Tenemos un problema de clasificación que tiene 5 clases. Queremos diseñar una red\n",
    "de neuronas, pero no tenemos claro cual será el modelo a elegir. Lo que si queremos\n",
    "es que la salida de la red nos devuelva la probabilidad de que un elemento pertenezca\n",
    "a cada una de las clases. Describe como sería la capa de salida de la red neuronal,\n",
    "cuantas neuronas de salida y de que tipo sería.\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "En un problema de clasificación con cinco clases en el que deseas obtener la probabilidad de pertenencia a cada clase, la capa de salida de la red neuronal generalmente constaría de cinco neuronas. Además, para lograr la salida de probabilidades, se utilizaría una función de activación softmax.\n",
    "\n",
    "**Capa de Salida:**\n",
    "- **Número de Neuronas:** Cinco neuronas, una para cada clase.\n",
    "- **Función de Activación:** Softmax.\n",
    "\n",
    "La función de activación softmax toma un vector de entrada y produce un vector de salida donde cada elemento representa la probabilidad de que la entrada pertenezca a una clase específica. La función softmax se define como:\n",
    "\n",
    "\\[ \\text{Softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}} \\]\n",
    "\n",
    "donde \\( z \\) es el vector de entrada, \\( C \\) es el número de clases y \\( i \\) es el índice de la clase.\n",
    "\n",
    "En resumen, la capa de salida de la red neuronal se configuraría para producir una distribución de probabilidad sobre las cinco clases, permitiéndote interpretar la salida como las probabilidades de pertenencia a cada clase para un dato de entrada dado. La clase con la probabilidad más alta sería la predicción final de la red neuronal.\n",
    "\n",
    "### Respuesta Isma\n",
    "para 5 salidas tendríamos 5 neuronas y la función de activación sería softmax.\n",
    "\n",
    "### Respuesta Diego\n",
    "Debería haber 5 neuronas en la capa de salida, una por cada clase. La función\n",
    "de activación más adecuada para este caso es la función Softmax. Mientras que\n",
    "funciones como la sigmoide comprimen las salidas entre 0 y 1, la Softmax es\n",
    "ideal para la clasificación multiclase porque no solo comprime las salidas entre\n",
    "0 y 1, sino que también asegura que la suma total de las salidas\n",
    "(probabilidades) de todas las neuronas en la capa sea igual a 1. Esto convierte\n",
    "efectivamente las salidas en una distribución de probabilidad\n",
    "\n",
    "### Claves de la respuesta\n",
    "Softmax sirve para dar probabilidades.\n",
    "\n",
    "## Problema 9\n",
    "1. ¿Dónde aplicarías clustering? Pon un ejemplo.\n",
    "2. Si utilizas K-Means, ¿Cómo podrías determinar el valor de K?\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "1. **Aplicación de Clustering:**\n",
    "   - **Ejemplo:** Imagina que tienes un conjunto de datos de clientes de un centro comercial y deseas agruparlos en segmentos basados en su comportamiento de compra. Aquí, podrías aplicar técnicas de clustering para identificar grupos de clientes similares en términos de patrones de compra. Esto te permitiría personalizar estrategias de marketing o servicios para cada segmento, mejorando así la satisfacción del cliente y la eficacia de las estrategias comerciales.\n",
    "\n",
    "2. **Determinación del Valor de K en K-Means:**\n",
    "   - **Método del Codo (Elbow Method):** Una técnica común para determinar el valor óptimo de K en K-Means es el método del codo. Consiste en ejecutar el algoritmo K-Means con diferentes valores de K y calcular la suma de las distancias cuadradas intra-cluster (inertia) para cada valor de K. Luego, se busca el \"codo\" en la gráfica de la inercia en función de K. El codo representa el punto donde agregar más clusters no proporciona una mejora significativa en la inercia. Ese valor de K se elige como el número óptimo de clusters.\n",
    "\n",
    "   - **Silhouette Score:** Otra métrica utilizada es el Silhouette Score, que mide cuán similar es un objeto a su propio clúster en comparación con otros clústeres. Se busca el valor de K que maximiza el Silhouette Score.\n",
    "\n",
    "   - **Métodos Jerárquicos:** Si se prefiere un enfoque jerárquico, se puede utilizar un dendrograma para visualizar la estructura jerárquica del clustering y tomar decisiones sobre el número óptimo de clusters.\n",
    "\n",
    "La elección del valor de K a menudo implica un equilibrio entre la simplicidad del modelo y su capacidad para representar adecuadamente la estructura subyacente de los datos. La interpretación del problema y la evaluación de diversas métricas pueden ayudar en la toma de decisiones.\n",
    "\n",
    "### Respuesta Diego\n",
    "1. ¿Dónde aplicarías clustering? Pon un ejemplo.\n",
    "- El clustering es una técnica de aprendizaje no supervisado que se utiliza para\n",
    "agrupar datos similares en conjuntos o clusters. Se aplica en una amplia\n",
    "variedad de campos y situaciones para descubrir patrones y estructuras en\n",
    "datos. Aquí tienes un ejemplo de dónde podrías aplicar el clustering:\n",
    "Ejemplo: Segmentación de clientes en una tienda en línea\n",
    "Imagina que tienes una tienda en línea que vende una variedad de productos y\n",
    "deseas comprender mejor a tus clientes para ofrecerles una experiencia de\n",
    "compra más personalizada y efectiva. Puedes aplicar clustering de la siguiente\n",
    "manera:\n",
    "1. Recopilación de datos: Recolecta datos sobre tus clientes, como\n",
    "historiales de compras, categorías de productos que han comprado,\n",
    "frecuencia de compra, gasto promedio, etc.\n",
    "2. Selección de características: Selecciona las características relevantes de\n",
    "los clientes que deseas utilizar en el proceso de clustering, como la\n",
    "frecuencia de compra y el gasto promedio.\n",
    "3. Aplicación de clustering: Utiliza un algoritmo de clustering, como\n",
    "K-Means o Hierarchical Clustering, para agrupar a tus clientes en\n",
    "segmentos similares en función de las características seleccionadas.\n",
    "Cada grupo representará un segmento de clientes con comportamientos\n",
    "de compra similares.\n",
    "4. Análisis de resultados: Analiza los resultados del clustering para\n",
    "identificar patrones y características distintivas de cada grupo de\n",
    "clientes. Por ejemplo, podrías descubrir que un grupo de clientes realiza\n",
    "compras frecuentes pero de bajo valor, mientras que otro grupo realiza\n",
    "compras esporádicas pero de alto valor.\n",
    "5. Personalización: Utiliza la información obtenida de la segmentación\n",
    "para personalizar las ofertas, promociones y recomendaciones de\n",
    "productos para cada grupo de clientes. Por ejemplo, podrías enviar\n",
    "ofertas especiales a clientes que realizan compras frecuentes pero de\n",
    "bajo valor para incentivar compras más grandes.\n",
    "2. Si utilizas K-Means, ¿Cómo podrías determinar el valor de K?\n",
    "- Se podría utilizar el método del codo. Pasos:\n",
    "- Ejecutar K-Means: Se ejecuta el algoritmo K-Means en el conjunto de\n",
    "datos para un rango de valores de K (por ejemplo, K=1 a K=10).\n",
    "- Calcular la Varianza: Para cada K, se calcula la suma de las distancias al\n",
    "cuadrado de los puntos a su centroide más cercano, conocida como la\n",
    "varianza dentro de los clústeres o WCSS (Within-Cluster Sum of\n",
    "Squares).\n",
    "- Graficar los Resultados: Se crea una gráfica de línea donde el eje X\n",
    "representa el número de clústeres (K) y el eje Y representa el valor de la\n",
    "varianza correspondiente (WCSS).\n",
    "- Identificar el codo: Se busca el punto en la gráfica donde el descenso de\n",
    "la varianza/WCSS cambia de ser rápido a ser lento, lo que indica que\n",
    "agregar más clústeres no mejora significativamente la varianza dentro\n",
    "del clúster. Este punto se asemeja a un \"codo\" en la gráfica.\n",
    "\n",
    "### Claves de la respuesta\n",
    "Quiere K-means, tecnica del codo, coef. de silhouette y el ejemplo de los clientes.\n",
    "\n",
    "## Problema 10\n",
    "Dado los siguientes datos:\n",
    "```\n",
    "| A1 | A2 | A3 | A4 |\n",
    "|----|----|----|----|\n",
    "| 2  | 0  | 3  | 2  |\n",
    "| 1  | 1  | 1  | 0  |\n",
    "| 2  | 0  | 2  | 2  |\n",
    "| 0  | 2  | 1  | 1  |\n",
    "| 2  | 1  | 0  | 1  |\n",
    "| 2  | 0  | 3  | 0  |\n",
    "```\n",
    "Describe que clases generaría un algoritmo de clustering aglomerativo con distancia\n",
    "de Manhattan\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "Para aplicar un algoritmo de clustering aglomerativo con distancia de Manhattan, se inicia considerando cada punto como un cluster individual y se fusionan iterativamente los clusters más cercanos. La distancia entre dos clusters se mide según la distancia de Manhattan entre sus puntos más cercanos.\n",
    "\n",
    "Vamos a describir el proceso paso a paso para los datos proporcionados:\n",
    "\n",
    "**Datos:**\n",
    "```\n",
    "| A1 | A2 | A3 | A4 |\n",
    "|----|----|----|----|\n",
    "| 2  | 0  | 3  | 2  |\n",
    "| 1  | 1  | 1  | 0  |\n",
    "| 2  | 0  | 2  | 2  |\n",
    "| 0  | 2  | 1  | 1  |\n",
    "| 2  | 1  | 0  | 1  |\n",
    "| 2  | 0  | 3  | 0  |\n",
    "```\n",
    "\n",
    "**Paso 1:** Calcular las distancias entre todos los pares de puntos utilizando la distancia de Manhattan.\n",
    "\n",
    "**Paso 2:** Fusionar los dos clusters más cercanos en un nuevo cluster. Continuar este proceso hasta que todos los puntos estén en un solo cluster.\n",
    "\n",
    "**Posibles pasos intermedios (ejemplo simplificado):**\n",
    "```\n",
    "| Cluster 1 | Cluster 2 | Distancia |\n",
    "|-----------|-----------|-----------|\n",
    "| 1         | 2         | 3         |\n",
    "| 3         | 6         | 4         |\n",
    "| 4         | 5         | 2         |\n",
    "```\n",
    "\n",
    "Después de cada paso, los clusters se fusionan según la distancia mínima. Por ejemplo, en el primer paso, los puntos 1 y 2 se fusionan en un nuevo cluster. En el segundo paso, los clusters 3 y 6 se fusionan, y así sucesivamente.\n",
    "\n",
    "**Resultado Final:**\n",
    "```\n",
    "| Cluster Final |\n",
    "|---------------|\n",
    "| 1             |\n",
    "| 2             |\n",
    "| 3             |\n",
    "| 4             |\n",
    "| 5             |\n",
    "| 6             |\n",
    "```\n",
    "\n",
    "En este resultado, cada número representa un punto del conjunto de datos. El algoritmo de clustering aglomerativo ha agrupado los puntos en clusters basándose en la distancia de Manhattan, y los clusters finales están representados en la tabla \"Cluster Final\". Cada cluster representa una clase obtenida mediante el proceso de clustering aglomerativo.\n",
    "\n",
    "### Respuesta Diego \n",
    "1º Restamos las filas que parecen similares para obtener la fila con menos distancia\n",
    "- absoluto(F0 - F2) = 0 0 1 0\n",
    "2º Sumamos todos los valores\n",
    "- 0 + 0 + 1 + 0 = 1\n",
    "3º Como es la que menos distancia a dado entonces, usamos las filas F0 y F2 para\n",
    "calcular el punto medio entre las 2\n",
    "- (F0 + F2)/2 = (4 0 5 4)/2 = 2 0 2.5 2\n",
    "4º Quitamos las filas usadas F0 y F2 y metemos la fila fusionada donde queramos,\n",
    "quedando:\n",
    "A1 A2 A3 A4\n",
    "2 0 2.5 2\n",
    "2 0 3 2\n",
    "1 1 1 0\n",
    "2 0 2 2\n",
    "0 2 1 1\n",
    "2 1 0 1\n",
    "2 0 3 0\n",
    "\n",
    "### Claves de la respuesta\n",
    "Copiar y rezar\n",
    "\n",
    "## Problema 11\n",
    "Tenemos un NPC en un videojuego de lucha y queremos que aprenda a luchar de\n",
    "forma eficiente contra una IA ya programada. No disponemos de un dataset con\n",
    "información de partidas previas. Qué algoritmo utilizarías y describe brevemente y de\n",
    "forma aproximada como lo implementarías.\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "Para abordar el problema de hacer que un NPC aprenda a luchar de manera eficiente contra una IA ya programada sin un conjunto de datos previo, podrías considerar el uso de algoritmos de aprendizaje por refuerzo (RL), y en particular, podrías implementar un enfoque basado en Q-Learning.\n",
    "\n",
    "**Algoritmo a utilizar: Q-Learning**\n",
    "\n",
    "**Descripción aproximada de la implementación:**\n",
    "\n",
    "1. **Definición del entorno:**\n",
    "   - Identifica las acciones posibles que el NPC puede realizar durante una pelea (moverse, atacar, bloquear, etc.).\n",
    "   - Define el estado del entorno, que podría incluir información sobre la posición del NPC, la salud del NPC y del oponente, la distancia entre ellos, etc.\n",
    "\n",
    "2. **Estructura de Q-Table:**\n",
    "   - Crea una tabla Q que represente el valor de cada par (estado, acción). Inicializa la tabla con valores arbitrarios o todos a cero.\n",
    "\n",
    "3. **Exploración y explotación:**\n",
    "   - Implementa una estrategia para equilibrar la exploración del espacio de acciones y la explotación de acciones conocidas. Esto podría ser epsilon-greedy, donde el NPC elige una acción aleatoria con probabilidad epsilon y, de lo contrario, elige la acción con el valor Q más alto.\n",
    "\n",
    "4. **Iteraciones de aprendizaje:**\n",
    "   - En cada iteración del juego, el NPC realiza una acción en el entorno según su estrategia (exploración/explotación).\n",
    "   - Observa la recompensa obtenida y la nueva observación del entorno después de la acción.\n",
    "   - Actualiza el valor Q para la acción y estado actual utilizando la fórmula de actualización de Q-Learning:\n",
    "     \\[ Q(s, a) \\leftarrow (1 - \\alpha) \\cdot Q(s, a) + \\alpha \\cdot (r + \\gamma \\cdot \\max_{a'} Q(s', a')) \\]\n",
    "     donde \\( \\alpha \\) es la tasa de aprendizaje y \\( \\gamma \\) es el factor de descuento.\n",
    "\n",
    "5. **Entrenamiento continuo:**\n",
    "   - Repite las iteraciones de aprendizaje durante un número significativo de partidas para permitir que el NPC mejore su estrategia.\n",
    "\n",
    "6. **Evaluación y ajuste:**\n",
    "   - Evalúa el rendimiento del NPC contra la IA programada y ajusta los hiperparámetros (tasa de aprendizaje, factor de descuento, epsilon) según sea necesario.\n",
    "\n",
    "Es importante señalar que la implementación real dependerá del entorno específico del juego y de los detalles de la IA programada con la que estás trabajando. Además, el éxito de este enfoque puede depender de la complejidad del juego y de cómo se define el espacio de estado y acciones.\n",
    "\n",
    "### Claves de la respuesta\n",
    "Quiere que le hables de aprendizaje por refuerzo y Q-Learning\n",
    "\n",
    "## Problema 12\n",
    "¿Qué podemos hacer si el espacio de estados de un algoritmo de Q-Learning es\n",
    "demasiado grande? Razona la respuestae indica si se te ocurre al menos dos posibles\n",
    "soluciones.\n",
    "\n",
    "### Respuesta ChatGPT\n",
    "Cuando el espacio de estados de un algoritmo de Q-Learning es demasiado grande, puede resultar ineficiente o incluso impracticable explorar y almacenar los valores Q para cada par estado-acción. Aquí hay dos posibles soluciones para abordar este problema:\n",
    "\n",
    "1. **Discretización de los estados:**\n",
    "   - Una solución común es discretizar el espacio de estados. En lugar de considerar cada valor único en el espacio de estados, agrupa o discretiza las observaciones en intervalos más grandes. Esto reduce la complejidad del espacio de estados y permite construir y actualizar una tabla Q más manejable. Sin embargo, esta técnica puede llevar a la pérdida de información y a una aproximación menos precisa del valor Q en algunos casos.\n",
    "\n",
    "2. **Uso de aproximadores de función (Funciones Q Aproximadoras):**\n",
    "   - En lugar de almacenar todos los valores Q en una tabla, puedes utilizar aproximadores de función, como redes neuronales, para estimar los valores Q. Las funciones Q aproximadoras pueden generalizar a partir de un conjunto limitado de ejemplos y proporcionar estimaciones para estados no vistos anteriormente. Esto es particularmente útil cuando el espacio de estados es continuo o muy grande. Al utilizar técnicas como el aprendizaje profundo (Deep Q Networks - DQN), la red neuronal puede aprender representaciones más ricas y generalizadas del espacio de estados.\n",
    "\n",
    "Ambas soluciones tienen sus ventajas y desventajas. La elección entre ellas dependerá del contexto específico del problema y de las características del entorno. La discretización puede ser más adecuada para espacios de estados más simples, mientras que el uso de funciones Q aproximadoras, como DQN, puede ser más efectivo en entornos con espacios de estados más complejos y continuos.\n",
    "\n",
    "### Claves de la respuesta\n",
    "Las limitaciones de Q-Learning son que el espacio de estados debe ser finito, ya que es\n",
    "muy complicado que la búsqueda encuentre una política óptima si no lo es. Aun así, es\n",
    "posible que los resultados no sean muy buenos. \n",
    "\n",
    "1. Se puede hacer una aproximación mixta\n",
    "aprendiendo la tabla Q con aprendizaje supervisado para encontrar más fácilmente la\n",
    "solución.\n",
    "\n",
    "2. Otra aproximación para aprender la\n",
    "tabla Q es sin un modelo. Deep QLearning (DQN) utiliza esta\n",
    "aproximación y es la base de\n",
    "AlphaZero y AlphaStar de\n",
    "DeepMind. DQN sustituye la tabla\n",
    "Q por una red neuronal profunda,\n",
    "que predice el valor de Q para\n",
    "todos los posibles estados.\n",
    "Estas versiones son más inestables,\n",
    "los valores de Q oscilan mucho y se requiere una gran cantidad de datos para que\n",
    "converjan. Se usa la ecuación de Bellman para calcular los valores estimados y luego\n",
    "entrenar la red con ellos.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
