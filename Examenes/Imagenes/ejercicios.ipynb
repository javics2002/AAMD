{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79756688",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2500 into shape (25,25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1027\u001b[39m):\n\u001b[0;32m     43\u001b[0m     testImage \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39miloc[i, :]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 44\u001b[0m     imagen \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestImage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     imagenes\u001b[38;5;241m.\u001b[39mappend(imagen)\n\u001b[0;32m     47\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(imagenes[\u001b[38;5;241m600\u001b[39m], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2500 into shape (25,25)"
     ]
    }
   ],
   "source": [
    "from ann import cost, cost_regL2, backprop, backprop2, predict, feedForward\n",
    "from utils import checkNNGradients\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import loadmat, savemat\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "testData = loadmat('testset.mat', squeeze_me=True)\n",
    "trainData = loadmat('trainset.mat', squeeze_me=True)\n",
    "\n",
    "y_train = pd.DataFrame(trainData['y'])\n",
    "X_train = pd.DataFrame(trainData['X'])\n",
    "\n",
    "y_test = testData['ytest']\n",
    "X_test = testData['Xtest']\n",
    "\n",
    "\n",
    "# plt.imshow(imagenes[0])\n",
    "# plt.show()\n",
    "# plt.imshow(imagenes[1])\n",
    "# plt.show()\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "# Aplicar PCA para reducir la dimensionalidad de las imágenes\n",
    "# X_train_pca = PCA(n_components=625).fit_transform(X_train)\n",
    "# X_train_pca_df = pd.DataFrame(X_train_pca)\n",
    "\n",
    "imagenes = []\n",
    "\n",
    "for i in range(1027):\n",
    "    testImage = X_train.iloc[i, :].values\n",
    "    imagen = np.array(testImage).reshape(50, 50)\n",
    "    imagenes.append(imagen)\n",
    "\n",
    "plt.imshow(imagenes[600], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "layer_sizes = [X_train.shape[1], 100, y_train.shape[1]]\n",
    "epsilon = 0.12\n",
    "theta_list = []\n",
    "\n",
    "for i in range(len(layer_sizes) - 1):\n",
    "    theta_list.append(np.random.rand(layer_sizes[i + 1], layer_sizes[i] + 1) * 2 * epsilon - epsilon)\n",
    "\n",
    "iterations = 1000\n",
    "lambda_ = 1\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(iterations):\n",
    "    cost, grads = backprop(theta_list, X_train, y_train, lambda_)\n",
    "\n",
    "    for j in range(len(theta_list)):\n",
    "        theta_list[j] -= alpha * grads[j]\n",
    "\n",
    "predictions = predict(theta_list, X_train)\n",
    "\n",
    "# threshold = 0.5; #Umbral de calculo de prediccion\n",
    "# my_predict = np.array(predictions) >= threshold\n",
    "# my_predict = my_predict.astype(int)\n",
    "\n",
    "# Datos entrenamiento\n",
    "accuracy = accuracy_score(np.argmax(y_train, axis=1), predictions)\n",
    "print(f'Precisión del perceptrón: {accuracy * 100:.2f}%')\n",
    "\n",
    "conf_matrix = confusion_matrix(np.argmax(y_train, axis=1), predictions)\n",
    "print(f'Matriz de confusión:\\n {conf_matrix}')\n",
    "\n",
    "mse = mean_squared_error(np.argmax(y_train, axis=1), predictions)\n",
    "print(f'Error cuadratico medio: {mse:.2f} \\n')\n",
    "\n",
    "# Datos reales\n",
    "predictions = predict(theta_list, X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Precisión del perceptrón: {accuracy * 100:.2f}%')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(f'Matriz de confusión:\\n {conf_matrix}')\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Error cuadratico medio: {mse:.2f} \\n')\n",
    "\n",
    "report_sklearn = classification_report(y_test, predictions)\n",
    "print(report_sklearn)\n",
    "\n",
    "print(\"\\nExplicaciones:\")\n",
    "print(\"Precision: Es la proporción de instancias clasificadas como positivas que son realmente positivas.\")\n",
    "print(\"Recall: Es la proporción de instancias positivas que fueron correctamente clasificadas como positivas.\")\n",
    "print(\"F1-score: Es la media armónica ponderada de precision y recall. Es útil cuando hay desbalance de clases.\")\n",
    "print(\"Support: Es el número real de ocurrencias de la clase en el conjunto de prueba.\")\n",
    "print(\"Accuracy: Es la proporción de instancias correctamente clasificadas en el conjunto de prueba.\")\n",
    "print(\"Macro avg: Es el promedio no ponderado de precision, recall y f1-score para todas las clases.\")\n",
    "print(\"Weighted avg: Es el promedio ponderado por el soporte (número de instancias) de precision, recall y f1-score.\")\n",
    "\n",
    "# SVM\n",
    "print('------------------------------\\n')\n",
    "print('DATOS DEL SUPPORT VECTOR MACHINE (SVM)\\n')\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', C=1.0)\n",
    "svm_classifier.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Datos de test\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Precisión del SVM: {accuracy * 100:.2f}%')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(f'Matriz de confusión:\\n {conf_matrix}')\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Error cuadrático medio: {mse:.2f} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4a418",
   "metadata": {},
   "source": [
    "### Teoria\n",
    "\n",
    "Justificar los datos del perceptron respecto a SVM\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
